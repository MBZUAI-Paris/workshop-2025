<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.ico"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="MBZUAI, Workshop, AI">
    <title>MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods">
        <div class="top-left">
            <span class="title2">MBZUAI Workshop 2025:</span><br><br><br>
            <span class="title3">Foundations and Advances in Generative AI:<br><br>Theory and Methods</span>
            <!-- <span class="year">2025</span> -->
        </div>
        <div class="bottom-right">
            February 12-13, 2025<br>Paris, France
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href=".">Home</a>
            </td>
            <!-- <td class="navigation">
                <a title="Register for the Conference" href="registration">Registration</a>
            </td> -->
            <td class="navigation">
                <a title="Speaker List" href="speaker-list">Speaker List</a>
            </td>
            <td class="navigation">
                <a title="Conference Program" href="program-day-1">Program Day 1</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program" href="program-day-2">Program Day 2</a> 
            </td>
            <!-- <td class="navigation">
                <a title="Directions to the Conference" href="directions">Directions</a>
            </td> -->
            <!-- <td class="navigation">
                <a title="Conference Flyer" href="flyer">Flyer</a>
            </td> -->
        </tr>
    </table>

    <h2>Program on Thursday, February 13</h2>

    <table>
        <tr>
            <td class="date" rowspan="2">
                9:00am
            </td>
            <td class="title-special">
                Registration and Coffee &amp; Tea!
            </td>
        </tr>
        <tr>
            <td class="abstract">
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                9:30am
            </td>
            <td class="title">
                TBD 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="#">TBD</a> (TBD)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                10:10am
            </td>
            <td class="title">
                A Primer on Physics-informed Machine Learning
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://perso.lpsm.paris/~biau/">Gérard Biau</a> (Sorbonne Université)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Physics-informed machine learning typically integrates physical priors into the learning process by minimizing a loss function that includes both a data-driven term and a partial differential equation (PDE) regularization. Building on the formulation of the problem as a kernel regression task, we use Fourier methods to approximate the associated kernel, and propose a tractable estimator that minimizes the physics-informed risk function. We refer to this approach as physics-informed kernel learning (PIKL). This framework provides theoretical guarantees, enabling the quantification of the physical prior’s impact on convergence speed. We demonstrate the numerical performance of the PIKL estimator through simulations, both in the context of hybrid modeling and in solving PDEs. Additionally, we identify cases where PIKL surpasses traditional PDE solvers, particularly in scenarios with noisy boundary conditions. Joint work with Francis Bach (Inria, ENS), Claire Boyer (Université Paris-Saclay), and Nathan Doumèche (Sorbonne Université, EDF R&D). 
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                10:50am
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>
    
    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                11:00am
            </td>
            <td class="title">
                GFlowNets: A Novel Framework for Diverse Generation in Combinatorial and Continuous Spaces
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://mbzuai.ac.ae/study/faculty/salem-lahlou/">Salem Lahlou</a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Generative Flow Networks offer a framework for sampling from reward-proportional distributions in combinatorial and continuous spaces. They provide an alternative to established methods such as MCMC that suffer from slow mixing in high-dimensional spaces. By leveraging flow conservation principles, GFlowNets enable exploration in scenarios where the diversity of solutions is crucial, differing from traditional reinforcement learning and generative models. The framework has shown practical utility in molecular design, protein structure prediction, and Bayesian network discovery, particularly when dealing with noisy reward landscapes where maintaining sample diversity is essential. Recent works have also explored GFlowNets as a mechanism for improving the systematic exploration capabilities of large language models. This talk will present the theoretical foundations of GFlowNets and discuss current research directions in expanding their applications.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                11:40am
            </td>
            <td class="title">
                From Diffusion Models to Schrödinger Bridges
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://vdeborto.github.io/">Valentin De Bortoli</a> (Google DeepMind)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Diffusion models have revolutionized generative modeling. Conceptually, these methods define a transport mechanism from a noise distribution to a data distribution. Recent advancements have extended this framework to define transport maps between arbitrary distributions, significantly expanding the potential for unpaired data translation. However, existing methods often fail to approximate optimal transport maps, which are theoretically known to possess advantageous properties. In this talk, we will show how one can modify current methodologies to compute Schrödinger bridges—an entropy-regularized variant of dynamic optimal transport. We will demonstrate this methodology on a variety of unpaired data translation tasks.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                12:20pm
            </td>
            <td class="title-special">
                Lunch
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                13:30pm
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="#">TBD</a> (TBD)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                13:50pm
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://d-tiapkin.github.io/">Daniil Tiapkin</a> (École Polytechnique)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                14:10pm
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="#">TBD</a> (TBD)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                14:50pm
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                15:00pm
            </td>
            <td class="title">
                What's not an Autoregressive LLM?
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://ikekonglp.github.io/">Lingpeng Kong</a> (University of Hong Kong)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                This talk explores alternatives to autoregressive Large Language Models (LLMs), with a particular focus on discrete diffusion models. The presentation covers recent advances in non-autoregressive approaches to text generation, reasoning, and planning tasks. Key developments discussed include Reparameterized Discrete Diffusion Models (RDMs), which show promising results in machine translation and error correction, and applications of discrete diffusion to complex reasoning tasks like countdown games, Sudoku, and chess. The talk also examines sequence-to-sequence text diffusion models, as well as the novel Diffusion of Thoughts (DoTs) framework for chain-of-thought reasoning. These non-autoregressive approaches demonstrate competitive performance while offering potential advantages in terms of parallel processing and flexible generation patterns compared to traditional autoregressive models.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                16:20pm
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="#">TBD</a> (TBD)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>

    <footer>
        &copy; MBZUAI France Lab
        &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
    </footer>

</body>
</html>

