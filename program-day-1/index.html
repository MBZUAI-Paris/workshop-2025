<!DOCTYPE html>
<html lang='en'>

<head>
    <base href="..">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.ico"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="MBZUAI, Workshop, AI">
    <title>MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods</title>
</head>

<body>

    <div class="banner">
        <img src="assets/banner.jpg" alt="MBZUAI Workshop 2025: Foundations and Advances in Generative AI: Theory and Methods">
        <div class="top-left">
            <span class="title2">MBZUAI Workshop 2025:</span><br><br><br>
            <span class="title3">Foundations and Advances in Generative AI:<br><br>Theory and Methods</span>
            <!-- <span class="year">2025</span> -->
        </div>
        <div class="bottom-right">
            February 12-13, 2025<br>Paris, France
        </div>
    </div>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href=".">Home</a>
            </td>
            <!-- <td class="navigation">
                <a title="Register for the Conference" href="registration">Registration</a>
            </td> -->
            <td class="navigation">
                <a title="Speaker List" href="speaker-list">Speaker List</a>
            </td>
            <td class="navigation">
                <a title="Conference Program" href="program-day-1">Program Day 1</a> 
            </td>
            <td class="navigation">
                <a title="Conference Program" href="program-day-2">Program Day 2</a> 
            </td>
            <!-- <td class="navigation">
                <a title="Directions to the Conference" href="directions">Directions</a>
            </td> -->
            <!-- <td class="navigation">
                <a title="Conference Flyer" href="flyer">Flyer</a>
            </td> -->
        </tr>
    </table>

    <h2>Program on Wednesday, February 12</h2>

    <table>
        <tr>
            <td class="date" rowspan="2">
                9:00am
            </td>
            <td class="title-special">
                Registration and Coffee &amp; Tea!
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                09:30am
            </td>
            <td class="title">
                Opening Remarks
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://mbzuai.ac.ae/study/faculty/professor-eric-xing/">Eric Xing</a> (MBZUAI & Carnegie Mellon University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                10:10am
            </td>
            <td class="title">
                Statistical Methods for Assessing the Factual Accuracy of Large Language Models 
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://candes.su.domains/">Emmanuel Candès</a> (Stanford University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We present new statistical methods for obtaining validity guarantees on the output of large language models (LLMs). These methods enhance conformal prediction techniques to filter out claims/remove hallucinations while providing a finite-sample guarantee on the error rate of what it being presented to the user. This error rate is adaptive in the sense that it depends on the prompt to preserve the utility of the output by not removing too many claims.  We demonstrate performance on real-world examples. This is joint work with John Cherian and Isaac Gibbs.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                10:50am
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                11:00am
            </td>
            <td class="title">
                Auditing and Mitigating Biases in (compressed) Language Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://velcin.github.io/">Julien Velcin</a> (University of Lyon)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                The size of language models plays a critical role in their ability to address complex tasks in NLP. However such big LMs can be hard to deploy on edge devices which leads to the need of compressing LLMs. Recent studies have shown that compressing pretrained models can significantly influence the way they deal with various biases, such as biases related to fairness and model calibration. In this talk, I will provide an overview of recent research conducted at the ERIC Lab as part of the <a target="_blank" href="https://www.anr-dike.fr/">DIKé project</a>. In particular, We will see how important quantization can lead to calibration errors and alter the model's confidence in its predictions. Additionnally, I will discuss ongoing work on the alignement of LLMs with moral values.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                11:40am
            </td>
            <td class="title">
                Foundation Models in Biology, from Histo-pathology to Genomics
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://jpvert.github.io/">Jean-Philippe Vert</a> (Owkin)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Large self-supervised foundation models have boosted the capabilities of AI models in natural language processing and computer vision. Can they also boost our understanding of biology and help us improve diagnosis and find new treatments for diseases like cancer? In this talk I will present our efforts to train foundation models for histopathology images of tissues, and to connect visual observations to the underlying genomics of the cells, paving the way to biomedical innovation in diagnosis and precision medicine.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                12:20am
            </td>
            <td class="title">
                Exploiting Knowledge for Model-based Deep Music Generation
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://www.telecom-paris.fr/gael-richard">Gaël Richard</a> (Télécom Paris)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We will describe and illustrate the concept of hybrid (or model-based) deep learning for music generation. This paradigm refers here to models that associates data-driven and model-based approaches in a joint framework by integrating our prior knowledge about the data in more controllable deep models. In the music domain, prior knowledge can relate for instance to the production or propagation of sound (using an acoustic or physical model) or how music is composed or structured (using a musicological model). In this presentation, we will first illustrate the concept and potential of such model-based deep learning approaches and then describe in more details its application to unsupervised music separation with source production models, music timbre transfer with diffusion and symbolic music generation with transformers using structured informed positional encoding.
            </td>
        </tr>
    </table>

    <table>
        <tr>
            <td class="date" rowspan="2">
                12:20pm
            </td>
            <td class="title-special">
                Lunch
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>



    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                14:00am
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a> (Tsinghua University)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>


    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                14:40pm
            </td>
            <td class="title">
                TBD
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://misovalko.github.io/index.html">Michal Valko</a> (INRIA & Stealth Startup)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                TBD
            </td>
        </tr>
    </table>



    <table>
        <tr>
            <td class="date" rowspan="2">
                14:50pm
            </td>
            <td class="title-special">
                Coffee &amp; Tea Break
            </td>
        </tr>
        <tr>
            <td class="abstract">
                <!-- Kept for Spacing -->
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                15:30pm
            </td>
            <td class="title">
                Moshi: A Speech-text Foundation Model for Real-time Dialogue
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://ai.honu.io/">Alexandre Défossez</a> (Kyutai)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                We will discuss Moshi, our recently released model. Moshi is capable of full-duplex dialogue, e.g. it can both speak and listen at any time, offering the most natural speech interaction to date. Besides, Moshi is also multimodal, in particular it is able to leverage its inner text monologue to improve the quality of its generation. We will cover the design choices behind Moshi in particular the efficient joint sequence modeling permitted by RQ-Transformer, and the use of large scale synthetic instruct data.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                16:10pm
            </td>
            <td class="title">
                Feature-Conditioned Graph Generation using Latent Diffusion Models
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://users.uop.gr/~nikolentzos/">Giannis Nikolentzos</a> (University of Peloponnese)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. In this talk, I will present Neural Graph Generator, our recently released model which utilizes conditioned latent diffusion models for graph generation. The model employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. Overall, this work represents a shift in graph generation methodologies, offering a more practical and efficient solution for generating diverse graphs with specific characteristics.
            </td>
        </tr>
    </table>

    <table id="PUTSPEAKERNAMEHERE">
        <tr>
            <td class="date" rowspan="3">
                16:50pm
            </td>
            <td class="title">
                Redefining AI Reasoning: From Self-Guided Exploration to Causal Loops, and Transformer-GNN Fusion
            </td>
        </tr>
        <tr>
            <td class="speaker">
                <a target="_blank" href="https://mbzuai.ac.ae/study/faculty/martin-takac/">Martin Takáč</a> (MBZUAI)
            </td>
        </tr>
        <tr>
            <td class="abstract">
                In this talk, we explore three intertwined directions that collectively redefine how AI systems reason about complex tasks. First, we introduce Self-Guided Exploration (SGE), a prompting strategy that enables Large Language Models (LLMs) to autonomously generate multiple “thought trajectories” for solving combinatorial problems. Through iterative decomposition and refinement, SGE delivers significant performance gains on NP-hard tasks—showcasing LLMs’ untapped potential in reasoning, logistics and resource management problems. Next, we delve into the Self-Referencing Causal Cycle (ReCall), a mechanism that sheds new light on LLMs’ ability to recall prior context from future tokens. Contrary to the common belief that unidirectional token generation fundamentally restricts memory, ReCall illustrates how “cycle tokens” create loops in the training data, enabling models to overcome the notorious “reversal curse.” Finally, we present a Transformer-GNN fusion architecture that addresses Transformers’ limitations in processing graph-structured data. 
            </td>
        </tr>
    </table>

    <table class="plenary">
        <tr>
            <td class="date" rowspan="3">
                18:00pm
            </td>
            <td class="title">
                Poster Session at MBZUAI France Lab
            </td>
        </tr>
        <tr>
            <td class="speaker">
                To present a poster, please fill out the <a href="https://forms.gle/susJHsShLfFixjMG8" target="_blank">Google form</a> for review.
            </td>
        </tr>
        <tr>
            <td class="abstract">
                Workshop participants are invited to join the poster session at MBZUAI France Lab.<br>
                Address: 42 Rue Notre Dame des Victoires, 75002 Paris
            </td>
        </tr>
    </table>

    <footer>
        &copy; MBZUAI France Lab
        &nbsp;|&nbsp; Design by <a href="https://github.com/mikepierce">Mike Pierce</a>
    </footer>

</body>
</html>

